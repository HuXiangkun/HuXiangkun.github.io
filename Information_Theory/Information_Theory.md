# Information Theory
----
<font size=4>
## 1. Basic Concepts
- Entropy of a discrete random variable:
$$ H(X) = -\sum_{x \in X} p(x) \log p(x) $$
This definition can be interpreted as the expectation of $ g(X) $ where $ g(X) = -\log p(x)$ , e.g.
$$ H(X) = E[g(x)] $$














</font>